{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**OutLine**\n\n\nInput\n  ↓\nLayerNorm\n  ↓\nMulti-Head Self-Attention\n  ↓\nResidual Add\n  ↓\nLayerNorm\n  ↓\nMLP (Feed Forward)\n  ↓\nResidual Add\n","metadata":{}},{"cell_type":"markdown","source":"**Step 1: Scaled Dot-Product Attention (from scratch)**","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T21:32:21.508353Z","iopub.execute_input":"2026-01-30T21:32:21.508924Z","iopub.status.idle":"2026-01-30T21:32:24.979278Z","shell.execute_reply.started":"2026-01-30T21:32:21.508895Z","shell.execute_reply":"2026-01-30T21:32:24.978463Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self,embed_dim):\n        super().__init__()\n        self.embed_dim=embed_dim\n        self.q=nn.Linear(embed_dim,embed_dim)\n        self.k=nn.Linear(embed_dim,embed_dim)\n        self.v=nn.Linear(embed_dim,embed_dim)\n\n\n        self.scale=embed_dim ** 0.5\n\n\n    def forward(self,x):\n        Q=self.q(x)\n        K=self.k(x)\n        V=self.v(x)\n\n\n        score=torch.matmul(Q,K.transpose(-2,-1))/self.scale\n\n\n        attn=F.softmax(scores,dim=-1)\n        out=torch.matmul(attn,V)\n\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:03:02.478431Z","iopub.execute_input":"2026-01-30T22:03:02.479496Z","iopub.status.idle":"2026-01-30T22:03:02.484744Z","shell.execute_reply.started":"2026-01-30T22:03:02.479466Z","shell.execute_reply":"2026-01-30T22:03:02.484103Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self,embed_dim):\n        super().__init__()\n        self.embed_dim=embed_dim\n        self.q=nn.Linear(embed_dim,embed_dim)\n        self.k=nn.Linear(embed_dim,embed_dim)\n        self.v=nn.Linear(embed_dim,embed_dim)\n\n\n        self.scale=embed_dim ** 0.5\n\n\n    def forward(self,x):\n        Q=self.q(x)\n        K=self.k(x)\n        V=self.v(x)\n\n\n        score=torch.matmul(Q,K.transpose(-2,-1))/self.scale\n\n\n        attn=F.softmax(score,dim-1)\n        out=torch.matmul(attn,V)\n\n\n        return out\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2: Multi-Head Attention (manual, clean)**","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,embed_dim,num_heads):\n        super().__init__()\n\n\n        assert embed_dim % num_heads == 0\n\n\n        self.embed_dim=embed_dim\n        self.num_heads=num_heads\n        self.head_dim=embed_dim // num_heads\n\n        self.qkv=nn.Linear(embed_dim,embed_dim*3)\n        self.out=nn.Linear(embed_dim,embed_dim)\n\n\n    def forward(self,x):\n        B,N,D=x.shape\n\n\n        qkv=self.qkv(x)\n        qkv=qkv.reshape(B,N,3,self.num_heads,self.head_dim)\n        qkv=qkv.permute(2,0,3,1,4)\n\n\n        Q,K,V=qkv[0],qkv[1],qkv[2]\n\n\n        scores=(Q @ K.transpose(-2,-1))/(self.head_dim ** 0.5)\n        attn=F.softmax(scores,dim=-1)\n\n\n        out=attn @ V\n\n        out=out.transpose(1,2).reshape(B,N,D)\n\n\n        return self.out(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:10:30.073743Z","iopub.execute_input":"2026-01-30T22:10:30.074500Z","iopub.status.idle":"2026-01-30T22:10:30.080609Z","shell.execute_reply.started":"2026-01-30T22:10:30.074469Z","shell.execute_reply":"2026-01-30T22:10:30.079831Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,embed_dim,num_heads):\n        super().__init__()\n        assert embed_dim%num_heads==0\n\n\n        self.embed_dim=embed_dim\n        self.num_heads=num_heads\n        self.heads_dim=embed_dim//num_heads\n\n        self.qkv=nn.Linear(embed_dim,embed_dim*3)\n        self.out=nn.Linear(embed_dim,embed_dim)\n\n\n    def forward(self,x):\n        B,N,D=x.shape\n\n\n        self.qkv=self.qkv(x)\n        self.qkv=qkv.reshape(B,N,3,self.num_heads,self.head_dim)\n        self.qkv=qkv.permute(2,0,4,1,4)\n\n\n        Q,K,V=qkv[0],qkv[1],qkv[2]\n\n\n        score=torch.matmul(Q @ K.transpose(-2,-1))/(self.num_heads ** 0.5)\n        attn=F.softmax(score,dim=1)\n\n\n        out= attn @ V\n        out=out.transpose(-2,-1).reshape(B,N,D)\n\n\n        return self.out(out)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*Step 3: Feed Forward Network (MLP block)*","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self,embed_dim,hidden_dim):\n        super().__init__()\n        self.net=nn.Sequential(\n            nn.Linear(embed_dim,hidden_dim),\n            nn.Gelu(),\n            nn.Linear(hidden_dim,embed_dim)\n        )\n\n\n    def forward(self,x):\n        return self.net(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:13:59.510222Z","iopub.execute_input":"2026-01-30T22:13:59.510542Z","iopub.status.idle":"2026-01-30T22:13:59.514857Z","shell.execute_reply.started":"2026-01-30T22:13:59.510516Z","shell.execute_reply":"2026-01-30T22:13:59.514230Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self,embed_dim,hidden_dim):\n        super().__init__()\n        self.net=nn.Sequential(\n            nn.Linear(embed_dim,hidden_dim)\n            nn.Gelu(),\n            nn.Linear(hidden_dim,hidden_dim)\n        )\n\n\n    def forward(self,x):\n        return self.net(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 4: Transformer Encoder Block (THIS IS HUGE)**","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self,embed_dim,num_heads,mlp_ratio=4):\n        super().__init__()\n        self.norm1=nn.LayerNorm(embed_dim)\n        self.attn=MultiHeadAttention(embed_dim,num_heads)\n\n\n        self.norm2=nn.LayerNorm(embed_dim)\n        self.mlp=FeedForward(embed_dim,embed_dim*mlp_ratio)\n\n\n    def forward(self,x):\n        x=x + self.attn(self.norm1(x))\n\n        x=x+self.mlp(self.norm2(x))\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:21:37.058780Z","iopub.execute_input":"2026-01-30T22:21:37.059479Z","iopub.status.idle":"2026-01-30T22:21:37.063996Z","shell.execute_reply.started":"2026-01-30T22:21:37.059448Z","shell.execute_reply":"2026-01-30T22:21:37.063420Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self,embed_dim,num_heads,mlp_ratio=4):\n        super().__init__()\n\n\n        self.norm1=nn.LayerNorm(embed_dim)\n        self.attn=MultiHeadedAttention(embed_dim,num_heads)\n\n\n        self.norm2=nn.LayerNorm(embed_dim)\n        self.mlp=FeedForward(embed_dim,embed_dim*mlp_ratio)\n\n    def forward(self,x):\n        x=x + self.attn(self.norm1(x))\n        x=x + self.mlp(self.norm2(x))\n\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5: Stack blocks → Transformer Encoder**","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self,depth,embed_dim,num_heads):\n        super().__init__()\n\n\n        self.layers=nn.ModuleList([\n            TransformerBlock(embed_dim,num_heads)\n            for _ in range(depth)\n        ])\n\n\n    def forward(self,x):\n        for layer in self.layers:\n            x=layer(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T22:24:44.792879Z","iopub.execute_input":"2026-01-30T22:24:44.793209Z","iopub.status.idle":"2026-01-30T22:24:44.798066Z","shell.execute_reply.started":"2026-01-30T22:24:44.793179Z","shell.execute_reply":"2026-01-30T22:24:44.797416Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self,depth,embed_dim,num_heads):\n        super().__init__()\n\n\n        self.layers=nn.ModuleList([\n            TransformerBlock(embed_dim,num_heads)\n            for _ in range(depth)\n        ])\n\n    def forward(self,x):\n        for layers in self.layers:\n            x=layer(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}