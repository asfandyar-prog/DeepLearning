{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: MLP, CNN and Vision Transformers in PyTorch"
      ],
      "metadata": {
        "id": "cXCe5QpcIUgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle setup (Colab)\n",
        "\n",
        "1) Open Kaggle → **Click Your PFP (top-right)** → **Settings** → **Account** → **API** → **Create Legacy API Key**  \n",
        "   This downloads `kaggle.json`.\n",
        "\n",
        "2) In Colab, run the next code cell. It will:\n",
        "- ask you to upload file... do so by clicking `Choose Files` and select `kaggle.json`\n",
        "- place it in the correct folder\n",
        "- set permissions\n",
        "- download + unzip the dataset\n"
      ],
      "metadata": {
        "id": "m7FFXmWOojoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install kaggle\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Upload kaggle.json\n",
        "uploaded = files.upload()\n",
        "assert \"kaggle.json\" in uploaded, \"Please upload kaggle.json\"\n",
        "\n",
        "# Move to the correct Kaggle folder\n",
        "kaggle_dir = Path(\"/root/.kaggle\")\n",
        "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "(Path(\"kaggle.json\")).replace(kaggle_dir / \"kaggle.json\")\n",
        "\n",
        "# Fix permissions (required by Kaggle)\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Download + unzip dataset\n",
        "!kaggle datasets download -d datamunge/sign-language-mnist -p /content/data --unzip\n",
        "\n",
        "!ls -lah /content/data\n"
      ],
      "metadata": {
        "id": "Bi9yu_byogwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset\n",
        "\n",
        "This dataset is provided as CSVs:\n",
        "- `sign_mnist_train.csv`\n",
        "- `sign_mnist_test.csv`\n",
        "\n",
        "Each row contains:\n",
        "- `label` (class id)\n",
        "- 784 pixel columns (28×28 flattened)\n",
        "\n",
        "We’ll split the Kaggle train CSV into train/val so we can compare models fairly.\n"
      ],
      "metadata": {
        "id": "i6Ubm87Tpmmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "train_path = \"/content/data/sign_mnist_train.csv\"\n",
        "test_path  = \"/content/data/sign_mnist_test.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df  = pd.read_csv(test_path)\n",
        "\n",
        "class SignLanguageMNIST(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.y = df[\"label\"].astype(np.int64).values\n",
        "        self.x = df.drop(columns=[\"label\"]).astype(np.float32).values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx] / 255.0\n",
        "        x = torch.tensor(x, dtype=torch.float32).view(1, 28, 28)\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "num_classes = 26\n",
        "CLASS_NAMES = [chr(ord(\"A\") + i) for i in range(26)]  # A..Z\n",
        "\n",
        "\n",
        "full_train = SignLanguageMNIST(train_df)\n",
        "test_ds    = SignLanguageMNIST(test_df)\n",
        "\n",
        "# split train into train/val\n",
        "val_frac = 0.15\n",
        "val_size = int(len(full_train) * val_frac)\n",
        "train_size = len(full_train) - val_size\n",
        "\n",
        "g = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds = random_split(full_train, [train_size, val_size], generator=g)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "eI2RkIJGprAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Sign Language MNIST\n",
        "\n",
        "We’ll:\n",
        "- map numeric labels → letters (the dataset excludes **J** and **Z** because they require motion) :contentReference[oaicite:0]{index=0}\n",
        "- show a small image grid\n",
        "- show class distribution (counts per label)\n"
      ],
      "metadata": {
        "id": "2p6hCbf0uGup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Label → Letter mapping (A..Z)\n",
        "LETTERS = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
        "assert num_classes == 26, f\"For A–Z mapping, set num_classes=26 (got {num_classes})\"\n",
        "\n",
        "def label_to_letter(y: int) -> str:\n",
        "    return LETTERS[int(y)]\n",
        "\n",
        "# --- Show class counts (force all 26 bars, even if some are zero)\n",
        "counts = train_df[\"label\"].value_counts().sort_index()\n",
        "all_counts = np.zeros(26, dtype=int)\n",
        "for k, v in counts.items():\n",
        "    all_counts[int(k)] = int(v)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(np.arange(26), all_counts)\n",
        "plt.title(\"Train class distribution (A–Z index space)\")\n",
        "plt.xlabel(\"label id\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.xticks(np.arange(26), LETTERS, rotation=0)\n",
        "plt.show()\n",
        "\n",
        "# --- Show a grid of examples\n",
        "def show_grid(ds, n=25):\n",
        "    n = int(n)\n",
        "    idxs = np.random.choice(len(ds), size=n, replace=False)\n",
        "    cols = int(np.sqrt(n))\n",
        "    rows = int(np.ceil(n / cols))\n",
        "\n",
        "    plt.figure(figsize=(cols*2.0, rows*2.0))\n",
        "    for i, idx in enumerate(idxs, 1):\n",
        "        x, y = ds[idx]\n",
        "        img = x.squeeze(0).numpy()\n",
        "\n",
        "        ax = plt.subplot(rows, cols, i)\n",
        "        ax.imshow(img, cmap=\"gray\")\n",
        "        ax.set_title(f\"{int(y)}:{label_to_letter(int(y))}\")\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_grid(full_train, n=25)"
      ],
      "metadata": {
        "id": "c4_E-2TQuFkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training utilities\n",
        "\n",
        "These functions work for MLP, CNN, and ViT.  \n",
        "Your job is to implement the model classes only.\n"
      ],
      "metadata": {
        "id": "mAxgUrqSqCdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from time import time\n",
        "\n",
        "def acc(logits, y):\n",
        "    return (logits.argmax(1) == y).float().mean().item()\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None):\n",
        "    train = optimizer is not None\n",
        "    model.train(train)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += acc(logits, y) * bs\n",
        "        total_n    += bs\n",
        "\n",
        "    return total_loss / total_n, total_acc / total_n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    return run_epoch(model, loader, optimizer=None)\n",
        "\n",
        "def fit(model, train_loader, val_loader, epochs=5, lr=1e-3, wd=0.0):\n",
        "    model = model.to(device)\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        t0 = time()\n",
        "        tr_loss, tr_acc = run_epoch(model, train_loader, optim)\n",
        "        va_loss, va_acc = evaluate(model, val_loader)\n",
        "        print(f\"ep {ep:02d} | train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f} | {time()-t0:.1f}s\")\n"
      ],
      "metadata": {
        "id": "pICidwm9qDnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix\n",
        "\n",
        "After training a model, use this to see which letters it confuses most often."
      ],
      "metadata": {
        "id": "aAB2ButUuTow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_preds(model, loader):\n",
        "    model.eval()\n",
        "    ys, ps = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        pred = logits.argmax(dim=1).cpu().numpy()\n",
        "        ys.append(y.numpy())\n",
        "        ps.append(pred)\n",
        "    return np.concatenate(ys), np.concatenate(ps)\n",
        "\n",
        "def plot_confusion_matrix(model, loader, class_names, normalize=True, title=\"Confusion Matrix\"):\n",
        "    y_true, y_pred = get_preds(model, loader)\n",
        "\n",
        "    K = len(class_names)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(K))  # <-- key fix\n",
        "\n",
        "    if normalize:\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm = np.divide(\n",
        "            cm.astype(np.float32),\n",
        "            row_sums,\n",
        "            out=np.zeros_like(cm, dtype=np.float32),\n",
        "            where=(row_sums != 0)  # avoids NaNs for empty classes like J/Z\n",
        "        )\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    ticks = np.arange(K)\n",
        "    plt.xticks(ticks, class_names, rotation=90)\n",
        "    plt.yticks(ticks, class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage after training:\n",
        "# plot_confusion_matrix(model_cnn.to(device), val_loader, LETTERS, normalize=True, title=\"CNN (val) confusion matrix\")\n"
      ],
      "metadata": {
        "id": "4xhBUCUsuS5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 — Simple MLP (baseline)\n",
        "\n",
        "**Goal:** Build a baseline that flattens the image and uses fully-connected layers.\n",
        "\n",
        "Hints:\n",
        "- Flatten 1×28×28 → 784\n",
        "- 2–4 hidden layers\n",
        "- Use `nn.ReLU()` or `nn.GELU()`\n",
        "- Add `nn.Dropout(p)` if you want\n",
        "- Final layer must output `num_classes`\n"
      ],
      "metadata": {
        "id": "8098B5boqMN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        # TODO: create layers (recommended: nn.Sequential)\n",
        "        # self.net = nn.Sequential(\n",
        "        #     nn.Flatten(),\n",
        "        #     nn.Linear(28*28, 512),\n",
        "        #     nn.ReLU(),\n",
        "        #     ...\n",
        "        #     nn.Linear(..., num_classes)\n",
        "        # )\n",
        "        #\n",
        "        # Do not apply a Softmax activation in your model's final layer;\n",
        "        # it is applied internally within the nn.CrossEntropyLoss function for numerical stability.\n",
        "\n",
        "        # Commment this line after implementing your network\n",
        "        self.net = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.net is None:\n",
        "            raise NotImplementedError(\"Define self.net in __init__\")\n",
        "        return self.net(x)\n",
        "\n",
        "model_simple_mlp = SimpleMLP(num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "XaKLLcJuqO1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Training for MLP"
      ],
      "metadata": {
        "id": "YrVS4cbCuub4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit(model_simple_mlp, train_loader, val_loader, epochs=10, lr=1e-3, wd=1e-4)\n",
        "\n",
        "val_loss, val_acc = evaluate(model_simple_mlp.to(device), val_loader)\n",
        "print(f\"MLP val_loss: {val_loss:.4f}\")\n",
        "print(f\"MLP val_acc:  {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "n5JFPehoukEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Confusion Matrix"
      ],
      "metadata": {
        "id": "raKxSSNIu23f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_simple_mlp.to(device), val_loader, LETTERS, normalize=True, title=\"MLP (val) confusion matrix\")"
      ],
      "metadata": {
        "id": "0sP6duMOuks4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 — CNN\n",
        "\n",
        "**Goal:** Build a small CNN that should outperform the MLP.\n",
        "\n",
        "Hints (keep it simple):\n",
        "- Use 2–4 conv blocks\n",
        "- A conv block can be: `Conv2d -> ReLU -> (BatchNorm2d optional) -> MaxPool2d`\n",
        "- Increase channels: 32 → 64 → 128\n",
        "- Use `AdaptiveAvgPool2d((1,1))` before the classifier so you don't fight tensor sizes."
      ],
      "metadata": {
        "id": "RHEgHo6j4Dau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ConvBlock = a reusable CNN building block.\n",
        "\n",
        "    REQUIREMENT (for the assignment):\n",
        "    - Build this block using nn.Sequential\n",
        "    - You must decide which layers to include and in what order.\n",
        "\n",
        "    Suggested ingredients (pick what you want):\n",
        "    - nn.Conv2d(...)\n",
        "    - activation: nn.ReLU() or nn.GELU()\n",
        "    - optional: nn.BatchNorm2d(...)\n",
        "    - optional: nn.MaxPool2d(...)\n",
        "    - optional: nn.Dropout2d(...)\n",
        "\n",
        "    Hint: store the Sequential in self.block and call it in forward().\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, use_bn: bool = True, use_pool: bool = True):\n",
        "        super().__init__()\n",
        "        # TODO: create a Python list called `layers`\n",
        "        # TODO: append your Conv2d\n",
        "        # TODO: append your activation\n",
        "        # TODO: if use_bn: append BatchNorm2d\n",
        "        # TODO: if use_pool: append MaxPool2d\n",
        "        # TODO: wrap it into nn.Sequential and assign to self.block\n",
        "        self.block = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.block is None:\n",
        "            raise NotImplementedError(\"Build self.block (nn.Sequential) inside ConvBlock.__init__\")\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    SimpleCNN splits the network into:\n",
        "    - feature_extractor: everything BEFORE flatten (outputs feature maps)\n",
        "    - classifier: maps flattened features -> num_classes\n",
        "\n",
        "    REQUIREMENT (for the assignment):\n",
        "    - feature_extractor MUST be an nn.Sequential of multiple ConvBlocks (and optionally pooling / adaptive pooling)\n",
        "    - classifier MUST be an nn.Sequential of Linear layers (and optional dropout / activation)\n",
        "\n",
        "    Tips:\n",
        "    - Use increasing channels (example idea: 1->32->64->128), but you choose.\n",
        "    - If you add nn.AdaptiveAvgPool2d((1,1)) at the end of feature_extractor,\n",
        "      flatten becomes easy because shape becomes (B, C, 1, 1) -> (B, C).\n",
        "    - Do NOT apply Softmax at the end. nn.CrossEntropyLoss applies it internally.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        # TODO: build self.feature_extractor (nn.Sequential)\n",
        "        #   It should stack ConvBlocks like:\n",
        "        #     ConvBlock(...), ConvBlock(...), ConvBlock(...), ...\n",
        "        #   Optionally end with nn.AdaptiveAvgPool2d((1,1)).\n",
        "\n",
        "        # TODO: build self.classifier (nn.Sequential)\n",
        "        #   It should accept the flattened output of feature_extractor\n",
        "        #   and end with a Linear(..., num_classes)\n",
        "\n",
        "        self.feature_extractor = None\n",
        "        self.classifier = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.feature_extractor is None or self.classifier is None:\n",
        "            raise NotImplementedError(\"Define self.feature_extractor and self.classifier in __init__\")\n",
        "        x = self.feature_extractor(x)   # (B, C, H, W)\n",
        "        x = torch.flatten(x, 1)         # (B, C*H*W) or (B, C) if using AdaptiveAvgPool2d((1,1))\n",
        "        return self.classifier(x)       # (B, num_classes)\n",
        "\n",
        "model_cnn = SimpleCNN(num_classes=num_classes)"
      ],
      "metadata": {
        "id": "TtDq88oqsXVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Training for CNN"
      ],
      "metadata": {
        "id": "8VhPP2veu8kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit(model_cnn, train_loader, val_loader, epochs=8, lr=2e-3, wd=1e-4)\n",
        "print(\"CNN val:\", evaluate(model_cnn.to(device), val_loader))"
      ],
      "metadata": {
        "id": "X_4jCiVNu7_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Confusion Matrix"
      ],
      "metadata": {
        "id": "yHtBuyxgvC_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_cnn.to(device), val_loader, LETTERS, normalize=True, title=\"CNN (val) confusion matrix\")"
      ],
      "metadata": {
        "id": "Nr99IAhsvCgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare models on validation\n",
        "\n",
        "Run this after you have trained the both."
      ],
      "metadata": {
        "id": "K1pxXu3KtBv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "results[\"mlp\"] = evaluate(model_simple_mlp.to(device), val_loader)\n",
        "results[\"cnn\"] = evaluate(model_cnn.to(device), val_loader)\n",
        "\n",
        "for name, (loss, a) in results.items():\n",
        "    print(f\"{name:>4} | val loss {loss:.4f} | val acc {a:.4f}\")"
      ],
      "metadata": {
        "id": "Xm8Nq4ortDQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Helpers"
      ],
      "metadata": {
        "id": "Niow4Hb8F2dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "\n",
        "def preprocess_to_sign_mnist(img, out_size=28, invert=\"auto\", enhance_contrast=True):\n",
        "    \"\"\"\n",
        "    Takes a PIL image / np array / path and returns a tensor shaped (1, 1, 28, 28)\n",
        "    similar to Sign Language MNIST input.\n",
        "\n",
        "    Steps (MNIST-ish):\n",
        "    - convert to grayscale\n",
        "    - resize while preserving aspect ratio\n",
        "    - pad to square (white background)\n",
        "    - resize to 28x28\n",
        "    - optional invert (auto tries both and picks \"more ink\")\n",
        "    - normalize to [0,1]\n",
        "    \"\"\"\n",
        "    # --- load\n",
        "    if isinstance(img, str):\n",
        "        img = Image.open(img)\n",
        "    elif isinstance(img, np.ndarray):\n",
        "        img = Image.fromarray(img)\n",
        "    elif not isinstance(img, Image.Image):\n",
        "        raise TypeError(\"img must be a path (str), PIL.Image, or numpy array\")\n",
        "\n",
        "    # --- grayscale\n",
        "    img = img.convert(\"L\")\n",
        "\n",
        "    # --- optional contrast boost (helps phone photos)\n",
        "    if enhance_contrast:\n",
        "        img = ImageEnhance.Contrast(img).enhance(1.5)\n",
        "\n",
        "    # --- resize keeping aspect ratio, then pad to square\n",
        "    img = ImageOps.contain(img, (out_size, out_size))\n",
        "    w, h = img.size\n",
        "    pad_l = (out_size - w) // 2\n",
        "    pad_t = (out_size - h) // 2\n",
        "    pad_r = out_size - w - pad_l\n",
        "    pad_b = out_size - h - pad_t\n",
        "    img = ImageOps.expand(img, border=(pad_l, pad_t, pad_r, pad_b), fill=255)  # white background\n",
        "\n",
        "    # ensure exact size\n",
        "    img = img.resize((out_size, out_size), Image.BILINEAR)\n",
        "\n",
        "    def to_tensor(pil_img):\n",
        "        arr = np.array(pil_img).astype(np.float32)\n",
        "        arr = arr / 255.0\n",
        "        return torch.tensor(arr).view(1, 1, out_size, out_size)\n",
        "\n",
        "    if invert == \"never\":\n",
        "        x = to_tensor(img)\n",
        "    elif invert == \"always\":\n",
        "        x = to_tensor(ImageOps.invert(img))\n",
        "    else:\n",
        "        # auto: try both and choose the one that has \"more ink\" (darker pixels) in the center\n",
        "        x1 = to_tensor(img)\n",
        "        x2 = to_tensor(ImageOps.invert(img))\n",
        "        # score: mean intensity (lower means more dark ink). pick lower mean.\n",
        "        score1 = x1.mean().item()\n",
        "        score2 = x2.mean().item()\n",
        "        x = x1 if score1 < score2 else x2\n",
        "\n",
        "    return x, img  # tensor + the final 28x28 PIL image (for display)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_single_image(model, img, class_names=None, device=None, topk=3, invert=\"auto\"):\n",
        "    \"\"\"\n",
        "    model: your trained model\n",
        "    img: path / PIL / np array\n",
        "    class_names: list of class labels to display (length must match model outputs)\n",
        "    device: \"cuda\" or \"cpu\" (defaults to whatever model is on)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    x, img28 = preprocess_to_sign_mnist(img, out_size=28, invert=invert)\n",
        "    x = x.to(device)\n",
        "\n",
        "    logits = model(x)                  # (1, C)\n",
        "    probs = torch.softmax(logits, dim=1).squeeze(0)  # (C,)\n",
        "\n",
        "    topk = min(int(topk), probs.numel())\n",
        "    vals, idxs = torch.topk(probs, k=topk)\n",
        "\n",
        "    results = []\n",
        "    for p, i in zip(vals.tolist(), idxs.tolist()):\n",
        "        label = class_names[i] if class_names is not None else str(i)\n",
        "        results.append((label, i, p))\n",
        "\n",
        "    return results, img28\n"
      ],
      "metadata": {
        "id": "6dGMeo4rF443"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single-image inference (upload a photo)\n",
        "\n",
        "Upload any hand-sign image (photo/screenshot).  \n",
        "This cell will:\n",
        "1) upload the image\n",
        "2) convert it into a **28×28 grayscale MNIST-like** tensor\n",
        "3) run your trained model on it\n",
        "4) show the processed 28×28 image + top predictions\n",
        "\n",
        "**Note:** `CLASS_NAMES` must match your model output size:\n",
        "- if `num_classes = 26`: `CLASS_NAMES = [\"A\", \"B\", ..., \"Z\"]`\n",
        "- if `num_classes = 25`: `CLASS_NAMES = [\"A\", ..., \"Y\"]`\n"
      ],
      "metadata": {
        "id": "hNYn-NiwFpVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Make sure these exist in your notebook already:\n",
        "# - preprocess_to_sign_mnist(...)\n",
        "# - predict_single_image(...)\n",
        "# - a trained model, e.g. model_cnn or model_simple_mlp\n",
        "# - device\n",
        "\n",
        "# 1) Upload image\n",
        "uploaded = files.upload()\n",
        "img_path = next(iter(uploaded.keys()))  # first uploaded filename\n",
        "print(\"Uploaded:\", img_path)\n",
        "\n",
        "# 2) Class names (adjust if your num_classes differs)\n",
        "# If you used num_classes = 26:\n",
        "CLASS_NAMES = [chr(ord(\"A\") + i) for i in range(26)]\n",
        "\n",
        "# 3) Pick a model to use (change this to whichever you trained)\n",
        "model_for_inference = model_cnn  # or model_simple_mlp, model_vit, etc.\n",
        "\n",
        "# 4) Predict\n",
        "results, img28 = predict_single_image(\n",
        "    model_for_inference.to(device),\n",
        "    img_path,\n",
        "    class_names=CLASS_NAMES,\n",
        "    topk=5,\n",
        "    invert=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"\\nTop predictions:\")\n",
        "for label, idx, prob in results:\n",
        "    print(f\"- {label} (class {idx}): {prob:.3f}\")\n",
        "\n",
        "# 5) Show the processed 28x28 input that the model actually saw\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img28, cmap=\"gray\")\n",
        "plt.title(\"Processed 28×28 input\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AsMilawjFobn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus (Optional)"
      ],
      "metadata": {
        "id": "MwHQkKK4GLYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny ViT for 28×28 grayscale (Step-by-step)\n",
        "\n",
        "Idea:\n",
        "1) Split image into patches (like “tokens” for vision)\n",
        "2) Embed each patch into a vector of size `dim`\n",
        "3) Add a `[CLS]` token + positional embeddings\n",
        "4) Pass tokens through a Transformer Encoder\n",
        "5) Use `[CLS]` output to classify into `num_classes`\n",
        "\n",
        "Recommended beginner-friendly setup:\n",
        "- `patch_size = 7` → (28/7 = 4) → **16 tokens** (smaller sequence, easier)\n",
        "- `dim = 128`, `depth = 3`, `heads = 4`\n",
        "\n",
        "(You *can* use patch_size=4 too, but that makes 49 tokens and can feel harder.)"
      ],
      "metadata": {
        "id": "kJ9p4MRPvZi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 — Patch Embedding\n",
        "\n",
        "We want:\n",
        "- Input:  x of shape (B, 1, 28, 28)\n",
        "- Output: tokens of shape (B, N, dim)\n",
        "\n",
        "Using Conv2d:\n",
        "- `Conv2d(1, dim, kernel_size=patch, stride=patch)`\n",
        "gives output (B, dim, H', W') where:\n",
        "- H' = 28/patch\n",
        "- W' = 28/patch\n",
        "- N = H'*W'\n",
        "\n",
        "Then we flatten:\n",
        "- (B, dim, H', W') → (B, dim, N) → (B, N, dim)"
      ],
      "metadata": {
        "id": "gsrCaojKv4xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, dim=128, patch_size=7):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # TODO: conv that produces patch embeddings\n",
        "        # self.proj = nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.proj = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.proj is None:\n",
        "            raise NotImplementedError(\"TODO: define self.proj in __init__\")\n",
        "\n",
        "        # TODO:\n",
        "        # x = self.proj(x)            # (B, dim, H', W')\n",
        "        # x = x.flatten(2)            # (B, dim, N)\n",
        "        # x = x.transpose(1, 2)       # (B, N, dim)\n",
        "        # return x\n",
        "        raise NotImplementedError(\"TODO: implement PatchEmbed.forward\")\n"
      ],
      "metadata": {
        "id": "xLbnH_qQvzXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — CLS token + Positional embeddings\n",
        "\n",
        "Transformers need positional info because attention alone doesn’t know order.\n",
        "\n",
        "We’ll create:\n",
        "- `cls_token`: (1, 1, dim) learnable\n",
        "- `pos_embed`: (1, 1+N, dim) learnable\n",
        "\n",
        "Forward:\n",
        "- tokens: (B, N, dim)\n",
        "- cls:    expand → (B, 1, dim)\n",
        "- concat: (B, 1+N, dim)\n",
        "- add pos: (B, 1+N, dim)"
      ],
      "metadata": {
        "id": "A73GrZI-wYE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddClsPos(nn.Module):\n",
        "    def __init__(self, num_patches: int, dim=128):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.dim = dim\n",
        "\n",
        "        # TODO: define cls token + pos embedding as nn.Parameter\n",
        "        # self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "        # self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, dim))\n",
        "        self.cls_token = None\n",
        "        self.pos_embed = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, N, dim)\n",
        "        if self.cls_token is None or self.pos_embed is None:\n",
        "            raise NotImplementedError(\"TODO: define cls_token and pos_embed\")\n",
        "\n",
        "        # TODO:\n",
        "        # B = x.size(0)\n",
        "        # cls = self.cls_token.expand(B, -1, -1)   # (B, 1, dim)\n",
        "        # x = torch.cat([cls, x], dim=1)           # (B, 1+N, dim)\n",
        "        # x = x + self.pos_embed                   # (B, 1+N, dim)\n",
        "        # return x\n",
        "        raise NotImplementedError(\"TODO: implement AddClsPos.forward\")\n"
      ],
      "metadata": {
        "id": "AD8HAJtpwcaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Transformer Encoder\n",
        "\n",
        "PyTorch gives you:\n",
        "- `nn.TransformerEncoderLayer`\n",
        "- `nn.TransformerEncoder`\n",
        "\n",
        "Key detail: set `batch_first=True` so we can keep (B, S, E).\n",
        "\n",
        "Input to encoder:\n",
        "- (B, 1+N, dim)\n",
        "Output:\n",
        "- (B, 1+N, dim)"
      ],
      "metadata": {
        "id": "040eNukAwe_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEncoder(nn.Module):\n",
        "    def __init__(self, dim=128, depth=3, heads=4, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # TODO: build encoder stack\n",
        "        # ff_dim = int(dim * mlp_ratio)\n",
        "        # layer = nn.TransformerEncoderLayer(\n",
        "        #     d_model=dim, nhead=heads,\n",
        "        #     dim_feedforward=ff_dim,\n",
        "        #     dropout=dropout,\n",
        "        #     activation=\"gelu\",\n",
        "        #     batch_first=True\n",
        "        # )\n",
        "        # self.encoder = nn.TransformerEncoder(layer, num_layers=depth)\n",
        "        self.encoder = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.encoder is None:\n",
        "            raise NotImplementedError(\"TODO: define self.encoder in __init__\")\n",
        "        return self.encoder(x)\n"
      ],
      "metadata": {
        "id": "j6h5ZVmewlrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Full TinyViT\n",
        "\n",
        "Pipeline:\n",
        "1) PatchEmbed:   (B, 1, 28, 28) → (B, N, dim)\n",
        "2) AddClsPos:    (B, N, dim)    → (B, 1+N, dim)\n",
        "3) TokenEncoder: (B, 1+N, dim)  → (B, 1+N, dim)\n",
        "4) Head on CLS:  use x[:,0]     → (B, dim) → (B, num_classes)"
      ],
      "metadata": {
        "id": "xSedY3l0wjjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyViT(nn.Module):\n",
        "    def __init__(self, num_classes: int, dim=128, depth=3, heads=4, patch_size=7, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert 28 % patch_size == 0, \"patch_size must divide 28 cleanly\"\n",
        "        num_patches = (28 // patch_size) * (28 // patch_size)\n",
        "\n",
        "        self.patch = PatchEmbed(dim=dim, patch_size=patch_size)\n",
        "        self.add_cp = AddClsPos(num_patches=num_patches, dim=dim)\n",
        "        self.enc = TokenEncoder(dim=dim, depth=depth, heads=heads, dropout=dropout)\n",
        "\n",
        "        # TODO: classifier head\n",
        "        # self.head = nn.Linear(dim, num_classes)\n",
        "        self.head = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.head is None:\n",
        "            raise NotImplementedError(\"TODO: define self.head in __init__\")\n",
        "\n",
        "        x = self.patch(x)     # (B, N, dim)\n",
        "        x = self.add_cp(x)    # (B, 1+N, dim)\n",
        "        x = self.enc(x)       # (B, 1+N, dim)\n",
        "\n",
        "        # TODO: take CLS token and classify\n",
        "        # cls = x[:, 0]        # (B, dim)\n",
        "        # return self.head(cls)\n",
        "        raise NotImplementedError(\"TODO: finish TinyViT.forward\")\n"
      ],
      "metadata": {
        "id": "NWg6r1RdwxvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug: check shapes with one batch\n",
        "\n",
        "Run this after you implement the TODOs in PatchEmbed / AddClsPos / TokenEncoder / TinyViT.\n",
        "It helps you catch shape mistakes early."
      ],
      "metadata": {
        "id": "HGLhU66-xRCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_vit_shapes(model, loader):\n",
        "    model = model.to(device).eval()\n",
        "    x, y = next(iter(loader))\n",
        "    x = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "\n",
        "    print(\"Input:\", tuple(x.shape))\n",
        "    print(\"Output:\", tuple(out.shape))\n",
        "    assert out.ndim == 2, \"Model output should be (B, num_classes)\"\n",
        "    assert out.shape[1] == num_classes, \"Second dim must be num_classes\"\n",
        "\n",
        "# Example:\n",
        "# model_vit = TinyViT(num_classes=num_classes, dim=128, depth=3, heads=4, patch_size=7)\n",
        "# debug_vit_shapes(model_vit, train_loader)\n"
      ],
      "metadata": {
        "id": "UI2b2mNHxR-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train TinyViT (once implemented)\n",
        "\n",
        "ViT often likes:\n",
        "- lower LR (e.g. 3e-4)\n",
        "- higher weight decay (e.g. 1e-2 to 5e-2)\n",
        "\n",
        "Start small and only increase depth/dim if it’s stable."
      ],
      "metadata": {
        "id": "L63RF4RaxYxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vit = TinyViT(num_classes=num_classes, dim=128, depth=3, heads=4, patch_size=7, dropout=0.1)\n",
        "fit(model_vit, train_loader, val_loader, epochs=12, lr=3e-4, wd=1e-2)\n",
        "print(\"ViT val:\", evaluate(model_vit.to(device), val_loader))"
      ],
      "metadata": {
        "id": "grH0ETKfxWSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Confusion Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "i-p38Gvkxg0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_vit.to(device), val_loader, LETTERS, normalize=True, title=\"ViT confusion matrix\")"
      ],
      "metadata": {
        "id": "v63ZvtptGPs2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}